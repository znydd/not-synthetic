{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e7ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned 'data/rel_new.txt' and saved to 'data/rel.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_file(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Loads a text file, replaces the string 'json' with a comma, and\n",
    "    removes other specific patterns. It then saves the cleaned content.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the input .txt file.\n",
    "        output_filepath (str): The path where the cleaned content will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the content of the input file\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Step 1: Replace the literal string 'json' with a comma\n",
    "        content_with_comma = content.replace('json', ',')\n",
    "\n",
    "        # Step 2: Define the regex pattern to find all other unwanted strings and URL types.\n",
    "        patterns_to_remove = re.compile(\n",
    "            r'```|'  # Matches the literal string ```\n",
    "            r'https?://(?:www\\.)?youtube\\.com/watch\\?v=[\\w-]+|'  # Matches standard youtube watch URLs\n",
    "            r'https?://youtu\\.be/[\\w-]+|'  # Matches shortened youtu.be URLs\n",
    "            r'\\[|\\]'  # Matches literal [ and ]\n",
    "        )\n",
    "\n",
    "        # Use re.sub to replace all occurrences of the patterns with an empty string\n",
    "        cleaned_content = patterns_to_remove.sub('', content_with_comma)\n",
    "\n",
    "        # Remove any extra whitespace created by the removal\n",
    "        cleaned_content = re.sub(r'\\s+', ' ', cleaned_content).strip()\n",
    "\n",
    "        # Write the cleaned content to the output file\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "\n",
    "        print(f\"Successfully cleaned '{input_filepath}' and saved to '{output_filepath}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filepath}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a file named 'input.txt' in the same directory.\n",
    "# This will create a new file named 'output.txt' with the cleaned content.\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'data/rel_new.txt'\n",
    "    output_file = 'data/rel.txt'\n",
    "    clean_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f256ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df27f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the schema with your fields\n",
    "my_schema = pa.schema([\n",
    "    pa.field(\"video_topic\", pa.string()),\n",
    "    pa.field(\"segment_description\", pa.string()),\n",
    "    pa.field(\"subtitle\", pa.string()),\n",
    "    pa.field(\"label\", pa.string())\n",
    "])\n",
    "\n",
    "# Create an empty table by providing a dictionary with empty lists for each field\n",
    "empty_table = pa.Table.from_pydict({\n",
    "    \"video_topic\": [],\n",
    "    \"segment_description\": [],\n",
    "    \"subtitle\": [],\n",
    "    \"label\": []\n",
    "}, schema=my_schema)\n",
    "\n",
    "# Write the empty table to a Parquet file\n",
    "pq.write_table(empty_table, 'relevant.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a44b1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eaab41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Define the schema of your Parquet file (must match the existing file)\n",
    "my_schema = pa.schema([\n",
    "    pa.field(\"video_topic\", pa.string()),\n",
    "    pa.field(\"segment_description\", pa.string()),\n",
    "    pa.field(\"subtitle\", pa.string()),\n",
    "    pa.field(\"label\", pa.string())\n",
    "])\n",
    "with open(\"data/rel.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "data_dict = {\n",
    "    \"video_topic\": [],\n",
    "    \"segment_description\": [],\n",
    "    \"subtitle\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "for entry in data:\n",
    "    data_dict['video_topic'].append(entry[\"video_topic\"])\n",
    "    data_dict[\"segment_description\"].append(entry[\"segment_description\"])\n",
    "    if entry['subtitle'] == \"\":\n",
    "        data_dict[\"subtitle\"].append(random.choice([\"**No one is talking**\", \"**Silent**\",\"**No subtitle as silent**\" ]))\n",
    "    else:\n",
    "        data_dict['subtitle'].append(entry['subtitle'])\n",
    "\n",
    "    data_dict[\"label\"].append(entry[\"label\"])\n",
    "\n",
    "# Create a ParquetWriter instance to append to the file\n",
    "writer = pq.ParquetWriter('relevant.parquet', my_schema)\n",
    "# Create a PyArrow Table with the data you want to add\n",
    "new_data = pa.Table.from_pydict(data_dict)\n",
    "# Write the new data to the file\n",
    "writer.write_table(new_data)\n",
    "\n",
    "# Close the writer to save and finalize the file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82e9c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Irrelevant Parquet file has 2602 rows.\n",
      "The Relevant Parquet file has 2012 rows.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    irrelevant_parquet_file = pq.ParquetFile(\"irrelevant.parquet\")\n",
    "    relevant_parquet_file = pq.ParquetFile(\"relevant.parquet\")\n",
    "    print(f\"The Irrelevant Parquet file has {irrelevant_parquet_file.metadata.num_rows} rows.\")\n",
    "    print(f\"The Relevant Parquet file has {relevant_parquet_file.metadata.num_rows} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5b948",
   "metadata": {},
   "source": [
    "### Preparing the Data for trainint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b589c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Parquet files...\n",
      "Relevant data: 2012 rows\n",
      "Irrelevant data: 2602 rows\n",
      "Shuffling datasets...\n",
      "Sample size - Relevant: 201, Irrelevant: 260\n",
      "\n",
      "=== Final Dataset Split Summary ===\n",
      "Training set: 3692 rows\n",
      "  - Relevant: 1610\n",
      "  - Irrelevant: 2082\n",
      "Validation set: 461 rows\n",
      "  - Relevant: 201\n",
      "  - Irrelevant: 260\n",
      "Test set: 461 rows\n",
      "  - Relevant: 201\n",
      "  - Irrelevant: 260\n",
      "\n",
      "Total rows: 4614\n",
      "Training: 80.0%\n",
      "Validation: 10.0%\n",
      "Test: 10.0%\n",
      "\n",
      "All splits saved to 'data' directory\n",
      "\n",
      "=== Split Creation Complete ===\n",
      "Files created:\n",
      "- train_set.parquet (main training set)\n",
      "- validation_set.parquet (main validation set)\n",
      "- test_set.parquet (main test set)\n",
      "- Plus individual class files for each split\n"
     ]
    }
   ],
   "source": [
    "# first shuffle both separate then 12% irr, 8% rel for test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_data_splits(relevant_file, irrelevant_file, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits from relevant and irrelevant Parquet files.\n",
    "    \n",
    "    Args:\n",
    "        relevant_file (str): Path to relevant.parquet file\n",
    "        irrelevant_file (str): Path to irrelevant.parquet file\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing the six DataFrames (train/val/test for both classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load the Parquet files\n",
    "    print(\"Loading Parquet files...\")\n",
    "    relevant_df = pd.read_parquet(relevant_file)\n",
    "    irrelevant_df = pd.read_parquet(irrelevant_file)\n",
    "    \n",
    "    print(f\"Relevant data: {len(relevant_df)} rows\")\n",
    "    print(f\"Irrelevant data: {len(irrelevant_df)} rows\")\n",
    "    \n",
    "    \n",
    "    # Shuffle both datasets\n",
    "    print(\"Shuffling datasets...\")\n",
    "    relevant_shuffled = relevant_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    irrelevant_shuffled = irrelevant_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate 5% sample sizes for each class\n",
    "    relevant_sample_size = max(1, int(0.1 * len(relevant_shuffled)))  # At least 1 sample\n",
    "    irrelevant_sample_size = max(1, int(0.1 * len(irrelevant_shuffled)))  # At least 1 sample\n",
    "    \n",
    "    print(f\"Sample size - Relevant: {relevant_sample_size}, Irrelevant: {irrelevant_sample_size}\")\n",
    "    \n",
    "    # Create validation set (5% from each class)\n",
    "    relevant_val = relevant_shuffled.iloc[:relevant_sample_size]\n",
    "    irrelevant_val = irrelevant_shuffled.iloc[:irrelevant_sample_size]\n",
    "    validation_set = pd.concat([relevant_val, irrelevant_val], ignore_index=True)\n",
    "    \n",
    "    # Create test set (next 5% from each class)\n",
    "    relevant_test = relevant_shuffled.iloc[relevant_sample_size:relevant_sample_size*2]\n",
    "    irrelevant_test = irrelevant_shuffled.iloc[irrelevant_sample_size:irrelevant_sample_size*2]\n",
    "    test_set = pd.concat([relevant_test, irrelevant_test], ignore_index=True)\n",
    "    \n",
    "    # The remaining data goes to training set\n",
    "    relevant_train = relevant_shuffled.iloc[relevant_sample_size*2:]\n",
    "    irrelevant_train = irrelevant_shuffled.iloc[irrelevant_sample_size*2:]\n",
    "    training_set = pd.concat([relevant_train, irrelevant_train], ignore_index=True)\n",
    "    \n",
    "    # Shuffle the final sets\n",
    "    validation_set = validation_set.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    test_set = test_set.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    training_set = training_set.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Final Dataset Split Summary ===\")\n",
    "    print(f\"Training set: {len(training_set)} rows\")\n",
    "    print(f\"  - Relevant: {len(training_set[training_set['label'] == 'Relevant'])}\")\n",
    "    print(f\"  - Irrelevant: {len(training_set[training_set['label'] == 'Irrelevant'])}\")\n",
    "    \n",
    "    print(f\"Validation set: {len(validation_set)} rows\")\n",
    "    print(f\"  - Relevant: {len(validation_set[validation_set['label'] == 'Relevant'])}\")\n",
    "    print(f\"  - Irrelevant: {len(validation_set[validation_set['label'] == 'Irrelevant'])}\")\n",
    "    \n",
    "    print(f\"Test set: {len(test_set)} rows\")\n",
    "    print(f\"  - Relevant: {len(test_set[test_set['label'] == 'Relevant'])}\")\n",
    "    print(f\"  - Irrelevant: {len(test_set[test_set['label'] == 'Irrelevant'])}\")\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_rows = len(training_set) + len(validation_set) + len(test_set)\n",
    "    print(f\"\\nTotal rows: {total_rows}\")\n",
    "    print(f\"Training: {len(training_set)/total_rows*100:.1f}%\")\n",
    "    print(f\"Validation: {len(validation_set)/total_rows*100:.1f}%\")\n",
    "    print(f\"Test: {len(test_set)/total_rows*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train': training_set,\n",
    "        'validation': validation_set,\n",
    "        'test': test_set,\n",
    "    }\n",
    "\n",
    "def save_splits(splits, output_dir='output_splits'):\n",
    "    \"\"\"\n",
    "    Save the splits as Parquet files.\n",
    "    \n",
    "    Args:\n",
    "        splits (dict): Dictionary containing the split DataFrames\n",
    "        output_dir (str): Directory to save the output files\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Save main splits\n",
    "    splits['train'].to_parquet(f'{output_dir}/train_set.parquet', index=False)\n",
    "    splits['validation'].to_parquet(f'{output_dir}/validation_set.parquet', index=False)\n",
    "    splits['test'].to_parquet(f'{output_dir}/test_set.parquet', index=False)\n",
    "    \n",
    "    # Save individual class splits (optional)\n",
    "    # splits['train_relevant'].to_parquet(f'{output_dir}/train_relevant.parquet', index=False)\n",
    "    # splits['train_irrelevant'].to_parquet(f'{output_dir}/train_irrelevant.parquet', index=False)\n",
    "    # splits['val_relevant'].to_parquet(f'{output_dir}/val_relevant.parquet', index=False)\n",
    "    # splits['val_irrelevant'].to_parquet(f'{output_dir}/val_irrelevant.parquet', index=False)\n",
    "    # splits['test_relevant'].to_parquet(f'{output_dir}/test_relevant.parquet', index=False)\n",
    "    # splits['test_irrelevant'].to_parquet(f'{output_dir}/test_irrelevant.parquet', index=False)\n",
    "    \n",
    "    print(f\"\\nAll splits saved to '{output_dir}' directory\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths - adjust these as needed\n",
    "    relevant_file = \"data/relevant.parquet\"\n",
    "    irrelevant_file = \"data/irrelevant.parquet\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(relevant_file):\n",
    "        print(f\"Error: {relevant_file} not found!\")\n",
    "        exit(1)\n",
    "    if not os.path.exists(irrelevant_file):\n",
    "        print(f\"Error: {irrelevant_file} not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    try:\n",
    "        # Create the splits\n",
    "        splits = create_data_splits(relevant_file, irrelevant_file, random_seed=42)\n",
    "        \n",
    "        # Save the splits\n",
    "        save_splits(splits, \"data\")\n",
    "        \n",
    "        print(\"\\n=== Split Creation Complete ===\")\n",
    "        print(\"Files created:\")\n",
    "        print(\"- train_set.parquet (main training set)\")\n",
    "        print(\"- validation_set.parquet (main validation set)\") \n",
    "        print(\"- test_set.parquet (main test set)\")\n",
    "        print(\"- Plus individual class files for each split\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2568923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1587ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "{'video_topic': 'An educational guide on how to prepare and deliver a webinar presentation for the ENG091 course.', 'segment_description': 'This extended segment is dedicated to administrative and logistical tasks. The instructor explains the rules for the upcoming group presentation, divides the students into groups, takes attendance, and addresses various student questions related to the assignment and group formations. This entire portion deviates from the academic subject of the lecture.', 'subtitle': \"and then what are we going to do for a webinar presentation in English 091? First I'm going to divide you into four groups not five groups four groups. Then you will see there are four articles on buX and I will assign one article to each group. Then, every participant will present the different main ideas or components of the article. So let's say this is a long article, right? This is probably an article of let's say 10 or 15 paragraphs, alright because the length of the article will vary. You you will see that. So you are not going to since it's a group presentation so not that everybody is going to do the presentation on the whole article. We you will follow a special reading strategy that is called jigsaw strategy. So in jigsaw strategy what we do is there are group there are group members in one group right? So you will divide the long text or the long article into chunks or parts. Then each group member will take only one part or one chunk of the article. So let's say if there are since there are 25 students, so there are going to be six students in one group. Only one group will have seven students. So let's say if there are 15 paragraphs then you can divide the paragraphs by like this. Like one person will present on two paragraphs. So if there are 15 paragraphs then you know if there are seven group members then each of them will get two paragraphs, only one person will get five three paragraphs. right? if you divide uh 15 I mean by by seven. right? So all of the seven members will get two paragraphs, only one all of the six members out of seven will get uh two paragraphs, only one member the seventh member will get three paragraphs. So you will take different parts of the article not that you have to do the presentation on the full article. Of course you will read the whole article but then you will sit in a group and then you will decide who is going to present on which part or on which paragraph. Then you will present the main idea of that specific paragraph or component of the article. And then okay. So since you are going to present only one or two paragraphs so what you are going to do is you are going to take an article from an outside source that you think is relevant to your article. So take two articles from outside source. Now it can be from a book, it can be from an article I mean it can be an article from the internet, it can be from a journal anywhere. but just make sure it's a proper article. So let's say the article that I assign your group with is on climate change alright. So you have to find two more articles on climate change. maybe you can find the article in a book or on the internet or on the book I mean from the books or any websites or any journals anywhere you want. And then you will take relevant information from the article and join it with you know your presentation. add the information from other articles uh to your presentation. Is that clear class? who asked this? I missed out because there are so many like uh faces together. Who asked this? me. It's me. Tahmida. Okay Tahmida. Alright so yes you have to analyze because without analyzing you won't understand the text right? and it's the same for the other two articles that you are going to take right? From the next time I will give attendance just before I mean just 5 minutes before the class ends because I noticed that a lot of people leave after giving the attendance but they don't know that I notice that and I you know mark them absent. if anyone do not send me or email me the reason for his or her leaving. they don't know that I mark them absent. anyway is anyone ready yet? Is anyone ready now? Not yet I guess alright. Keep reading let me know when you are done. you can do a presentation now maybe on one or two paragraphs. you don't have to take the full article the whole article. Done class we don't have much time. Are you still reading? Yes? okay but I cannot see Munim, Nuzhat, Tabassum Bushra. uh then Muhammad Hossain, Mostofa, Shobuj, Istiak at least turn on the camera and let me give you attendance in the meanwhile. Shakib Shadman? I guess absent right? Shakib Shadman? Present ma'am present. Okay present. Istiak? Present ma'am. Yes Present ma'am. Present. Abdullah Sany okay. Mahmuda Tabassum Present ma'am. Okay. anyone ready? So anyone is ready? Are you reading? No ma'am. I'm still reading. Okay. Mahmuda Tabassum? No ma'am I'm also reading still reading. Okay Ifat Sultana what's your update? Uh ma'am I'm still reading too. Shakib Shadman still reading? Yes ma'am still reading. Farhin I cannot see you properly. Okay keep reading let me know when you are done. \", 'label': 'Irrelevant'}\n",
      "4644\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/irrelevant_full.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "mx = 0\n",
    "idx = 0\n",
    "for i, p in enumerate(data):\n",
    "    ln = len(p['subtitle'])\n",
    "    mx = max(ln, mx)\n",
    "    if(ln == 4644):\n",
    "        print(i)\n",
    "        print(p)\n",
    "print(mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b22f057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nabil\n",
      "zero\n"
     ]
    }
   ],
   "source": [
    "s = \"nabil\"+\"\\n\"+\"zero\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's take attendance now. All assignments submitted? Confirmed? Yes. Shakib Shadman? Present. Munim? Absent. Mostofa? Absent. Istiak? Yes. Tanjila? Absent. Nibir, Haque, present. Abdullah Sany? Absent. Mahmuda Tabassum? Absent. Khalid? Tanvir, Farhin, Atik, present. Tasnim, present. Nuzhat? Sabuj, present. Hosamuddin? Absent. Riyad, Shoyeb? Yes. Mahmuda joined. Iffat? Tabassum Raya? Sifat? Present. Tahmida, present. Tabassum Bushra? Mehedi, present.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
