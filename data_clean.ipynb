{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e7ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned 'data/rel_new.txt' and saved to 'data/rel.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_file(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Loads a text file, replaces the string 'json' with a comma, and\n",
    "    removes other specific patterns. It then saves the cleaned content.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the input .txt file.\n",
    "        output_filepath (str): The path where the cleaned content will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the content of the input file\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Step 1: Replace the literal string 'json' with a comma\n",
    "        content_with_comma = content.replace('json', ',')\n",
    "\n",
    "        # Step 2: Define the regex pattern to find all other unwanted strings and URL types.\n",
    "        patterns_to_remove = re.compile(\n",
    "            r'```|'  # Matches the literal string ```\n",
    "            r'https?://(?:www\\.)?youtube\\.com/watch\\?v=[\\w-]+|'  # Matches standard youtube watch URLs\n",
    "            r'https?://youtu\\.be/[\\w-]+|'  # Matches shortened youtu.be URLs\n",
    "            r'\\[|\\]'  # Matches literal [ and ]\n",
    "        )\n",
    "\n",
    "        # Use re.sub to replace all occurrences of the patterns with an empty string\n",
    "        cleaned_content = patterns_to_remove.sub('', content_with_comma)\n",
    "\n",
    "        # Remove any extra whitespace created by the removal\n",
    "        cleaned_content = re.sub(r'\\s+', ' ', cleaned_content).strip()\n",
    "\n",
    "        # Write the cleaned content to the output file\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "\n",
    "        print(f\"Successfully cleaned '{input_filepath}' and saved to '{output_filepath}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filepath}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a file named 'input.txt' in the same directory.\n",
    "# This will create a new file named 'output.txt' with the cleaned content.\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'data/rel_new.txt'\n",
    "    output_file = 'data/rel.txt'\n",
    "    clean_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f256ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/rel.json\") as f:\n",
    "    data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df27f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the schema with your fields\n",
    "my_schema = pa.schema([\n",
    "    pa.field(\"video_topic\", pa.string()),\n",
    "    pa.field(\"segment_description\", pa.string()),\n",
    "    pa.field(\"subtitle\", pa.string()),\n",
    "    pa.field(\"label\", pa.string())\n",
    "])\n",
    "\n",
    "# Create an empty table by providing a dictionary with empty lists for each field\n",
    "empty_table = pa.Table.from_pydict({\n",
    "    \"video_topic\": [],\n",
    "    \"segment_description\": [],\n",
    "    \"subtitle\": [],\n",
    "    \"label\": []\n",
    "}, schema=my_schema)\n",
    "\n",
    "# Write the empty table to a Parquet file\n",
    "pq.write_table(empty_table, 'relevant.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a44b1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eaab41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Define the schema of your Parquet file (must match the existing file)\n",
    "my_schema = pa.schema([\n",
    "    pa.field(\"video_topic\", pa.string()),\n",
    "    pa.field(\"segment_description\", pa.string()),\n",
    "    pa.field(\"subtitle\", pa.string()),\n",
    "    pa.field(\"label\", pa.string())\n",
    "])\n",
    "with open(\"data/rel.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "data_dict = {\n",
    "    \"video_topic\": [],\n",
    "    \"segment_description\": [],\n",
    "    \"subtitle\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "for entry in data:\n",
    "    data_dict['video_topic'].append(entry[\"video_topic\"])\n",
    "    data_dict[\"segment_description\"].append(entry[\"segment_description\"])\n",
    "    if entry['subtitle'] == \"\":\n",
    "        data_dict[\"subtitle\"].append(random.choice([\"**No one is talking**\", \"**Silent**\",\"**No subtitle as silent**\" ]))\n",
    "    else:\n",
    "        data_dict['subtitle'].append(entry['subtitle'])\n",
    "\n",
    "    data_dict[\"label\"].append(entry[\"label\"])\n",
    "\n",
    "# Create a ParquetWriter instance to append to the file\n",
    "writer = pq.ParquetWriter('relevant.parquet', my_schema)\n",
    "# Create a PyArrow Table with the data you want to add\n",
    "new_data = pa.Table.from_pydict(data_dict)\n",
    "# Write the new data to the file\n",
    "writer.write_table(new_data)\n",
    "\n",
    "# Close the writer to save and finalize the file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82e9c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Irrelevant Parquet file has 2602 rows.\n",
      "The Relevant Parquet file has 2012 rows.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    irrelevant_parquet_file = pq.ParquetFile(\"irrelevant.parquet\")\n",
    "    relevant_parquet_file = pq.ParquetFile(\"relevant.parquet\")\n",
    "    print(f\"The Irrelevant Parquet file has {irrelevant_parquet_file.metadata.num_rows} rows.\")\n",
    "    print(f\"The Relevant Parquet file has {relevant_parquet_file.metadata.num_rows} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049af59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not-synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
